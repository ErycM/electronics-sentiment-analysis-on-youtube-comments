{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Análise de sentimento - Comentários de produtos eletrônicos do youtube - Armazenamento no Firebase"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from datetime import datetime\r\n",
    "import pandas as pd\r\n",
    "import time\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "import stop_words\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\r\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.svm import SVC, LinearSVC\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.datasets import make_classification\r\n",
    "from scikitplot.metrics import plot_confusion_matrix\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\r\n",
    "from yellowbrick.regressor import ResidualsPlot\r\n",
    "\r\n",
    "import seaborn as sns\r\n",
    "import imblearn\r\n",
    "from collections import Counter\r\n",
    "from sklearn.datasets import make_classification\r\n",
    "from imblearn.over_sampling import SMOTE\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from imblearn.pipeline import Pipeline\r\n",
    "from matplotlib import pyplot\r\n",
    "from numpy import where\r\n",
    "\r\n",
    "\r\n",
    "from imblearn.over_sampling import RandomOverSampler\r\n",
    "from imblearn.over_sampling import SMOTE \r\n",
    "# from imblearn.over_sampling import SMOTENC\r\n",
    "# from imblearn.over_sampling import SMOTEN\r\n",
    "from imblearn.over_sampling import ADASYN \r\n",
    "from imblearn.over_sampling import BorderlineSMOTE\r\n",
    "from imblearn.over_sampling import KMeansSMOTE\r\n",
    "from imblearn.over_sampling import SVMSMOTE \r\n",
    "\r\n",
    "from sklearn.ensemble import AdaBoostClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn import svm\r\n",
    "\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from sklearn.decomposition import LatentDirichletAllocation,TruncatedSVD\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import re\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "from catboost import CatBoostClassifier, Pool"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\erycm\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-69bc4ce79a5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('video_comments_final_types.csv')\r\n",
    "df['final_type'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       " 1.0    3018\n",
       " 0.0    1578\n",
       "-1.0     236\n",
       "Name: final_type, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df['final_type'].loc[ (df['final_type'] == 1) | (df['final_type'] == 0) ] = 3\r\n",
    "# df['final_type'].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Transform"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#default transformation\r\n",
    "df['comment'] = df['comment'].astype(str)\r\n",
    "df['final_type'] = df['final_type'].astype(int)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_punctuation(dfText):\r\n",
    "    import re\r\n",
    "    import string\r\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\r\n",
    "\r\n",
    "    tokenized_docs_no_punctuation = []\r\n",
    "\r\n",
    "    for review in dfText:\r\n",
    "        # new_review = []\r\n",
    "        new_review = \"\"\r\n",
    "        for token in review:\r\n",
    "            new_token = regex.sub(u'', token)\r\n",
    "            if not new_token == u'':\r\n",
    "                #new_review.append(new_token)\r\n",
    "                new_review = new_review + new_token\r\n",
    "            else:\r\n",
    "                new_review = new_review + \" \"\r\n",
    "        \r\n",
    "        tokenized_docs_no_punctuation.append(new_review)\r\n",
    "    return tokenized_docs_no_punctuation\r\n",
    "\r\n",
    "def unicode_emoji(dfText):\r\n",
    "    import emoji\r\n",
    "    for emoj in emoji.UNICODE_EMOJI['pt']:\r\n",
    "        dfText = dfText.str.replace(emoj, ' '+emoji.UNICODE_EMOJI['pt'][emoj]+' ', regex=False)\r\n",
    "    return dfText\r\n",
    "\r\n",
    "def normalize_utf8(dfText):\r\n",
    "    return dfText.str.normalize(\"NFKD\").str.encode(\"ascii\", errors=\"ignore\").str.decode(\"utf8\")\r\n",
    "\r\n",
    "def removing_stop_words(dfText):\r\n",
    "    import nltk\r\n",
    "    nltk.download('stopwords')\r\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese') # removing stop words\r\n",
    "    \r\n",
    "    stopwords.append('q')\r\n",
    "    stopwords.append('pra')\r\n",
    "    stopwords.append('td')\r\n",
    "    stopwords.remove('não')\r\n",
    "\r\n",
    "    stopwords = pd.DataFrame(stopwords, columns=['normalized'])\r\n",
    "    stopwords['normalized'] = stopwords['normalized'].str.normalize(\"NFKD\").str.encode(\"ascii\", errors=\"ignore\").str.decode(\"utf8\")\r\n",
    "\r\n",
    "    stopword_data = []\r\n",
    "    for idx,review in enumerate(dfText):\r\n",
    "        new_phrase = \"\"\r\n",
    "        for word in review.split(\" \"):\r\n",
    "            # print(word)\r\n",
    "            if  not stopwords['normalized'].str.match('^'+word+'$').any():\r\n",
    "                new_phrase = new_phrase + \" \" + word\r\n",
    "\r\n",
    "        stopword_data.append(new_phrase)\r\n",
    "\r\n",
    "    return stopword_data\r\n",
    "\r\n",
    "def portuguese_stemmer(dfText):\r\n",
    "    # #!pip install git+git://github.com/snowballstem/pystemmer\r\n",
    "    import Stemmer\r\n",
    "    stemmer = Stemmer.Stemmer('portuguese')\r\n",
    "\r\n",
    "    stemmer_docs = []\r\n",
    "    for phrase in dfText:\r\n",
    "        stemmer_docs.append(' '.join(stemmer.stemWords(phrase.split(\" \"))))\r\n",
    "\r\n",
    "    return stemmer_docs\r\n",
    "\r\n",
    "def excess_space_remover(dfText):\r\n",
    "    all_commnets_list = dfText.to_list()\r\n",
    "\r\n",
    "    for i in range(len(all_commnets_list)):\r\n",
    "        all_commnets_list[i] = re.sub(r\"\\s+\", \" \", all_commnets_list[i])\r\n",
    "\r\n",
    "    return all_commnets_list\r\n",
    "\r\n",
    "def lower_case(dfText):\r\n",
    "    return dfText.str.lower()\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['transformed_comment'] = lower_case(df['comment']) \r\n",
    "df['transformed_comment'] = remove_punctuation(df['transformed_comment']) \r\n",
    "df['transformed_comment'] = unicode_emoji(df['transformed_comment'])\r\n",
    "df['transformed_comment'] = normalize_utf8(df['transformed_comment'])\r\n",
    "df['transformed_comment'] = removing_stop_words(df['transformed_comment'])\r\n",
    "df['transformed_comment'] = portuguese_stemmer(df['transformed_comment'])\r\n",
    "df['transformed_comment'] = excess_space_remover(df['transformed_comment'])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erycm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imbalance Apply"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def overSamplDef(X_res, y_res, overMethod, sampling_strategy='auto'):\r\n",
    "    from collections import Counter\r\n",
    "    from imblearn.over_sampling import RandomOverSampler\r\n",
    "    from imblearn.over_sampling import SMOTE \r\n",
    "    # from imblearn.over_sampling import SMOTENC\r\n",
    "    from imblearn.over_sampling import SMOTEN\r\n",
    "    from imblearn.over_sampling import ADASYN \r\n",
    "    from imblearn.over_sampling import BorderlineSMOTE\r\n",
    "    from imblearn.over_sampling import KMeansSMOTE\r\n",
    "    from imblearn.over_sampling import SVMSMOTE \r\n",
    "    \r\n",
    "    print(sampling_strategy)\r\n",
    "\r\n",
    "    print('Before dataset shape %s' % sorted(Counter(y_res).items()))\r\n",
    "    ros = overMethod(sampling_strategy=sampling_strategy)\r\n",
    "    # ros = BorderlineSMOTE()\r\n",
    "    # sampling_strategy='minority'\r\n",
    "    # ros = SMOTE()\r\n",
    "    X_res, y_res = ros.fit_resample(X_res, y_res)\r\n",
    "\r\n",
    "    print('Resampled dataset shape %s' % sorted(Counter(y_res).items()))\r\n",
    "    print(\"-------------------------------------------\")\r\n",
    "    return X_res, y_res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Freatures"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# required_columns = 'comment'\r\n",
    "required_columns = 'transformed_comment'\r\n",
    "le = LabelEncoder()\r\n",
    "\r\n",
    "X = df[required_columns]\r\n",
    "y = le.fit_transform(df['final_type'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_commnets_list = df[required_columns].to_list()\r\n",
    "\r\n",
    "tokenized_words = []\r\n",
    "\r\n",
    "for i in range(len(all_commnets_list)):\r\n",
    "    #tokenize the text to list of sentences\r\n",
    "    tokenized_sentence = nltk.sent_tokenize(all_commnets_list[i])\r\n",
    "    #tokenize the list of sentences to list of words\r\n",
    "    tokenized = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]\r\n",
    "    #remove the stop words from the text\r\n",
    "    for y, _ in enumerate(tokenized):\r\n",
    "        tokenized_words.append([word for word in tokenized[y]])\r\n",
    "\r\n",
    "all_commnets_list = tokenized_words\r\n",
    "\r\n",
    "model = Word2Vec(all_commnets_list, min_count=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.wv.save('eletronics_model.bin')\r\n",
    "# import fasttext.util\r\n",
    "embeddings = KeyedVectors.load('eletronics_model.bin')\r\n",
    "# embeddings = KeyedVectors.load('cc.pt.300.bin')\r\n",
    "# embeddings = fasttext.load_model(\"cc.pt.300.bin\")\r\n",
    "# all_commnets_list = np.array(all_commnets_list\r\n",
    "\r\n",
    "# model.build_vocab(\"cc.pt.300.bin\", update=True)\r\n",
    "\r\n",
    "# embeddings = model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word2vec_doc_vec = pd.DataFrame()\r\n",
    "for phrase in all_commnets_list:\r\n",
    "  temp = pd.DataFrame()\r\n",
    "\r\n",
    "  for word in phrase:\r\n",
    "    try:\r\n",
    "      word_vec = embeddings[word]\r\n",
    "      temp = temp.append(pd.Series(word_vec), ignore_index = True)\r\n",
    "    except:\r\n",
    "      pass\r\n",
    "  doc_vector = temp.mean()\r\n",
    "  word2vec_doc_vec = word2vec_doc_vec.append(doc_vector, ignore_index = True)\r\n",
    "\r\n",
    "word2vec_doc_vec.shape\r\n",
    "X_w2v = word2vec_doc_vec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# import pickle\r\n",
    "\r\n",
    "# pickle.dump(X_w2v, open('X_w2v.pkl', 'wb'))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_w2v' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f5b5bffacf5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_w2v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X_w2v.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_w2v' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tfidf_v = TfidfVectorizer(ngram_range = (3, 3))\r\n",
    "tfidf_v = TfidfVectorizer(ngram_range = (2, 3))\r\n",
    "#matrixTFIDF= tfidf_v.fit_transform(train.question_text)\r\n",
    "matrixTFIDF= tfidf_v.fit_transform(df[required_columns])\r\n",
    "svd=TruncatedSVD(n_components=100, n_iter=20, algorithm='randomized')\r\n",
    "X_lsa=svd.fit_transform(matrixTFIDF) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_lsa.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4832, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Traning Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v  = train_test_split(X_w2v, y, test_size = .2)\r\n",
    "# X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, RandomOverSampler) #SMOTE | SMOTE\r\n",
    "# clf = AdaBoostClassifier().fit(X_train_w2v, y_train_w2v)\r\n",
    "# y_pred_w2v = clf.predict(X_test_w2v)\r\n",
    "\r\n",
    "# X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, y, test_size = .2)\r\n",
    "# X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, RandomOverSampler) #SMOTE | SMOTE\r\n",
    "# clf = AdaBoostClassifier().fit(X_train_lsa, y_train_lsa)\r\n",
    "# y_pred_lsa = clf.predict(X_test_lsa)\r\n",
    "\r\n",
    "# Set the parameters by cross-validation\r\n",
    "# tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\r\n",
    "#                      'C': [1, 10, 100, 1000]},\r\n",
    "#                     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\r\n",
    "\r\n",
    "\r\n",
    "param_grid = [\r\n",
    "  {'C': [1, 10, 100, 1000]}\r\n",
    " ] \r\n",
    "svc = LinearSVC()\r\n",
    "# svc = LinearSVC(class_weight='balanced')\r\n",
    "\r\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, df['final_type'], test_size = .20)\r\n",
    "X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, ADASYN, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "model_w2v = GridSearchCV(svc, param_grid).fit(X_train_w2v, y_train_w2v)\r\n",
    "y_pred_w2v = model_w2v.predict(X_test_w2v)\r\n",
    "y_pred_train_w2v = model_w2v.predict(X_train_w2v)\r\n",
    "\r\n",
    "param_grid = [\r\n",
    "  {'C': [1, 10, 100, 1000]}\r\n",
    " ] \r\n",
    "svc = LinearSVC()\r\n",
    "\r\n",
    "X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, df['final_type'], test_size = .20)\r\n",
    "X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, ADASYN, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "model_lsa = GridSearchCV(svc, param_grid).fit(X_train_lsa, y_train_lsa)\r\n",
    "y_pred_lsa = model_lsa.predict(X_test_lsa)\r\n",
    "y_pred_train_lsa = model_lsa.predict(X_train_lsa)\r\n",
    "\r\n",
    "# X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v  = train_test_split(X_w2v, df['final_type'], test_size = .2)\r\n",
    "# X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, RandomOverSampler, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "\r\n",
    "# abc = AdaBoostClassifier(base_estimator=XGBClassifier())\r\n",
    "# parameters = {'base_estimator__max_depth':[i for i in range(2,11,2)],\r\n",
    "#               'base_estimator__min_samples_leaf':[5,10],\r\n",
    "#               'n_estimators':[10,50,250,1000],\r\n",
    "#               'learning_rate':[0.01,0.1]}\r\n",
    "\r\n",
    "# clf = GridSearchCV(abc, parameters,verbose=3,scoring='f1',n_jobs=-1)\r\n",
    "# clf.fit(X_train_w2v, y_train_w2v)\r\n",
    "# y_pred_w2v = clf.predict(X_test_w2v)\r\n",
    "\r\n",
    "# X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, df['final_type'], test_size = .2)\r\n",
    "# X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, RandomOverSampler, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "\r\n",
    "# abc = AdaBoostClassifier(base_estimator=XGBClassifier())\r\n",
    "# parameters = {'base_estimator__max_depth':[i for i in range(2,11,2)],\r\n",
    "#               'base_estimator__min_samples_leaf':[5,10],\r\n",
    "#               'n_estimators':[10,50,250,1000],\r\n",
    "#               'learning_rate':[0.01,0.1]}\r\n",
    "\r\n",
    "# clf = GridSearchCV(abc, parameters,verbose=3,scoring='f1',n_jobs=-1)\r\n",
    "# clf.fit(X_train_lsa, y_train_lsa)\r\n",
    "# y_pred_w2v = clf.predict(X_test_lsa)\r\n",
    "\r\n",
    "# X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v  = train_test_split(X_w2v, df['final_type'], test_size = .2)\r\n",
    "# X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, ADASYN, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "# clf = AdaBoostClassifier().fit(X_train_w2v, y_train_w2v)\r\n",
    "# y_pred_w2v = clf.predict(X_test_w2v)\r\n",
    "\r\n",
    "# X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, df['final_type'], test_size = .2)\r\n",
    "# X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, ADASYN, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "# clf = AdaBoostClassifier().fit(X_train_lsa, y_train_lsa)\r\n",
    "# y_pred_lsa = clf.predict(X_test_lsa)\r\n",
    "\r\n",
    "# from sklearn.linear_model import SGDClassifier\r\n",
    "# from sklearn.preprocessing import StandardScaler\r\n",
    "# from sklearn.pipeline import make_pipeline\r\n",
    "\r\n",
    "# X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v  = train_test_split(X_w2v, df['final_type'], test_size = .2)\r\n",
    "# # X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, RandomOverSampler) #SMOTE | SMOTE\r\n",
    "# clf = make_pipeline(StandardScaler(), SGDClassifier())\r\n",
    "# clf.fit(X_train_w2v, y_train_w2v)\r\n",
    "# y_pred_w2v = clf.predict(X_test_w2v)\r\n",
    "\r\n",
    "# X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, df['final_type'], test_size = .2)\r\n",
    "# X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, RandomOverSampler) #SMOTE | SMOTE\r\n",
    "# clf = make_pipeline(StandardScaler(), SGDClassifier())\r\n",
    "# clf.fit(X_train_lsa, y_train_lsa)\r\n",
    "# y_pred_lsa = clf.predict(X_test_lsa)\r\n",
    "\r\n",
    "# XGBClassifier\r\n",
    "\r\n",
    "# X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v  = train_test_split(X_w2v, df['final_type'], test_size = .2)\r\n",
    "# X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, RandomOverSampler) #SMOTE | SMOTE\r\n",
    "# clf = XGBClassifier(max_depth=3, n_estimators=25).fit(X_train_w2v, y_train_w2v)\r\n",
    "# y_pred_w2v = clf.predict(X_test_w2v)\r\n",
    "\r\n",
    "# X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, df['final_type'], test_size = .2)\r\n",
    "# X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, RandomOverSampler) #SMOTE | SMOTE\r\n",
    "# clf = XGBClassifier(max_depth=3, n_estimators=25).fit(X_train_lsa, y_train_lsa)\r\n",
    "# y_pred_lsa = clf.predict(X_test_lsa)\r\n",
    "\r\n",
    "# from catboost import CatBoostClassifier\r\n",
    "\r\n",
    "# X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v  = train_test_split(X_w2v, df['final_type'], test_size = .2)\r\n",
    "# X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, SMOTE, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "# model_w2v = CatBoostClassifier()\r\n",
    "# model_w2v.fit(X_train_w2v, y_train_w2v)\r\n",
    "# y_pred_w2v = model_w2v.predict(X_test_w2v)\r\n",
    "# y_pred_train_w2v = model_w2v.predict(X_train_w2v)\r\n",
    "\r\n",
    "# X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, df['final_type'], test_size = .2)\r\n",
    "# X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, SMOTE, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "# model_lsa = CatBoostClassifier()\r\n",
    "# model_lsa.fit(X_train_lsa, y_train_lsa)\r\n",
    "# y_pred_lsa = model_lsa.predict(X_test_lsa)\r\n",
    "# y_pred_train_lsa = model_lsa.predict(X_train_lsa)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minority\n",
      "Before dataset shape [(-1, 188), (0, 1272), (1, 2405)]\n",
      "Resampled dataset shape [(-1, 2403), (0, 1272), (1, 2405)]\n",
      "-------------------------------------------\n",
      "minority\n",
      "Before dataset shape [(-1, 196), (0, 1276), (1, 2393)]\n",
      "Resampled dataset shape [(-1, 2375), (0, 1276), (1, 2393)]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# target_names = ['Negativo', 'Neutro', 'Positivo']\r\n",
    "\r\n",
    "print(\"SVC - Report W2V\")\r\n",
    "print(classification_report(y_test_w2v, y_pred_w2v))\r\n",
    "\r\n",
    "print(\"SVC - Report LSA\")\r\n",
    "print(classification_report(y_test_lsa, y_pred_lsa))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVC - Report W2V\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.08      0.77      0.14        48\n",
      "           0       0.25      0.00      0.01       306\n",
      "           1       0.63      0.50      0.56       613\n",
      "\n",
      "    accuracy                           0.36       967\n",
      "   macro avg       0.32      0.43      0.24       967\n",
      "weighted avg       0.48      0.36      0.36       967\n",
      "\n",
      "SVC - Report LSA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.05      0.70      0.09        40\n",
      "           0       0.34      0.05      0.09       302\n",
      "           1       0.68      0.39      0.50       625\n",
      "\n",
      "    accuracy                           0.30       967\n",
      "   macro avg       0.36      0.38      0.23       967\n",
      "weighted avg       0.55      0.30      0.35       967\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model_report = pd.DataFrame()\r\n",
    "# predict_w2v_traning = []\r\n",
    "# predict_lsa_traning = []\r\n",
    "\r\n",
    "# for exec in range(600):\r\n",
    "#     param_grid = [\r\n",
    "#         {'C': [1, 10, 100, 1000]}\r\n",
    "#     ] \r\n",
    "#     svc = LinearSVC()\r\n",
    "\r\n",
    "#     X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, df['final_type'], test_size = .2)\r\n",
    "#     X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, ADASYN, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "#     clf = GridSearchCV(svc, param_grid).fit(X_train_w2v, y_train_w2v)\r\n",
    "#     y_pred_w2v = clf.predict(X_test_w2v)\r\n",
    "\r\n",
    "#     X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, df['final_type'], test_size = .2)\r\n",
    "#     X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, ADASYN, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "#     clf = GridSearchCV(svc, param_grid).fit(X_train_lsa, y_train_lsa)\r\n",
    "#     y_pred_lsa = clf.predict(X_test_lsa)\r\n",
    "\r\n",
    "#     svc_w2v = classification_report(y_test_w2v, y_pred_w2v, output_dict=True)\r\n",
    "#     predict_w2v_traning.append(svc_w2v)\r\n",
    "\r\n",
    "#     svc_lsa = classification_report(y_test_lsa, y_pred_lsa, output_dict=True)\r\n",
    "#     predict_lsa_traning.append(svc_lsa)\r\n",
    "\r\n",
    "# model_w2v_report = pd.json_normalize(predict_w2v_traning)\r\n",
    "# model_lsa_report = pd.json_normalize(predict_lsa_traning)\r\n",
    "\r\n",
    "# model_w2v_report.to_csv('w2v_report2.csv', index=False)\r\n",
    "# model_lsa_report.to_csv('lsa_report2.csv', index=False)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from multiprocessing import Process, Lock\r\n",
    "\r\n",
    "# def model_parallel(l, i):\r\n",
    "#     l.acquire()\r\n",
    "#     try:\r\n",
    "#         param_grid = [\r\n",
    "#             {'C': [1, 10, 100, 1000]}\r\n",
    "#         ] \r\n",
    "#         svc = LinearSVC()\r\n",
    "\r\n",
    "#         X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, df['final_type'], test_size = .2)\r\n",
    "#         X_train_w2v, y_train_w2v = overSamplDef(X_train_w2v, y_train_w2v, ADASYN, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "#         clf = GridSearchCV(svc, param_grid).fit(X_train_w2v, y_train_w2v)\r\n",
    "#         y_pred_w2v = clf.predict(X_test_w2v)\r\n",
    "\r\n",
    "#         param_grid = [\r\n",
    "#             {'C': [1, 10, 100, 1000]}\r\n",
    "#         ] \r\n",
    "#         svc = LinearSVC()\r\n",
    "\r\n",
    "#         X_train_lsa, X_test_lsa, y_train_lsa, y_test_lsa = train_test_split(X_lsa, df['final_type'], test_size = .2)\r\n",
    "#         X_train_lsa, y_train_lsa = overSamplDef(X_train_lsa, y_train_lsa, ADASYN, sampling_strategy='minority') #SMOTE | SMOTE\r\n",
    "#         clf = GridSearchCV(svc, param_grid).fit(X_train_lsa, y_train_lsa)\r\n",
    "#         y_pred_lsa = clf.predict(X_test_lsa)\r\n",
    "\r\n",
    "#         svc_w2v = classification_report(y_test_w2v, y_pred_w2v, output_dict=True)\r\n",
    "#         predict_w2v_traning.append(svc_w2v)\r\n",
    "\r\n",
    "#         svc_lsa = classification_report(y_test_lsa, y_pred_lsa, output_dict=True)\r\n",
    "#         predict_lsa_traning.append(svc_lsa)\r\n",
    "#     finally:\r\n",
    "#         l.release()\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# if __name__ == '__main__':\r\n",
    "#     lock = Lock()\r\n",
    "\r\n",
    "#     from multiprocessing import Process, Lock\r\n",
    "\r\n",
    "#     model_report = pd.DataFrame()\r\n",
    "#     predict_w2v_traning = []\r\n",
    "#     predict_lsa_traning = []\r\n",
    "\r\n",
    "#     for num in range(600):\r\n",
    "#         Process(target=model_parallel, args=(lock, num)).start()\r\n",
    "\r\n",
    "#     model_w2v_report = pd.json_normalize(predict_w2v_traning)\r\n",
    "#     model_lsa_report = pd.json_normalize(predict_lsa_traning)\r\n",
    "\r\n",
    "#     model_w2v_report.to_csv('w2v_report2.csv', index=False)\r\n",
    "#     model_lsa_report.to_csv('lsa_report2.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "e049b45738c4749feec37a8215276347ba84459169ad7790684fdcca7413c3f2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}